{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bdab2f-3e10-42e0-aef7-3e40d24f3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvetochek\\FastFit\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Tsvetochek\\FastFit\n",
    "\n",
    "import os\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# –ø–æ–¥–∫–ª—é—á–∏–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏\n",
    "from module.pipeline_fastfit import FastFitPipeline\n",
    "from parse_utils import DWposeDetector, DensePose, SCHP, multi_ref_cloth_agnostic_mask\n",
    "\n",
    "PERSON_SIZE = (768, 1024)\n",
    "DEF_SEED=12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211f7f03-2247-4a68-9099-04b6798b846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvetochek\\FastFit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch Models/FastFit-MR-1024: Error no file named diffusion_pytorch_model.safetensors found in directory Models/FastFit-MR-1024.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7880): –æ–±—ã—á–Ω–æ —Ä–∞–∑—Ä–µ—à–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ —Å–æ–∫–µ—Ç–∞ (–ø—Ä–æ—Ç–æ–∫–æ–ª/—Å–µ—Ç–µ–≤–æ–π –∞–¥—Ä–µ—Å/–ø–æ—Ä—Ç)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Tsvetochek\\FastFit\\ff_gradio.py\", line 367, in <module>\n",
      "    demo.launch(\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\fastfit\\lib\\site-packages\\gradio\\blocks.py\", line 2794, in launch\n",
      "    ) = http_server.start_server(\n",
      "  File \"C:\\ProgramData\\anaconda3\\envs\\fastfit\\lib\\site-packages\\gradio\\http_server.py\", line 157, in start_server\n",
      "    raise OSError(\n",
      "OSError: Cannot find empty port in range: 7880-7880. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Tsvetochek\\FastFit\n",
    "!python ff_gradio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1ecd6-613c-436c-9878-c6dbe2e71848",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --to script gradio.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1205c3b2-29ab-4a85-a73d-6de797fadbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–ª–æ–≤–∞—Ä—å\n",
    "# –í–µ—Å—å —Ç–µ–∫—Å—Ç, –æ–±—Ä–∞—â–µ–Ω–Ω—ã–π –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —Ö—Ä–∞–Ω–∏—Ç—Å—è –∑–¥–µ—Å—å –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "translations = {\n",
    "    \"title\": \"FastFit: –£—Å–∫–æ—Ä—å—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—É—é –ø—Ä–∏–º–µ—Ä–∫—É –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º —Å—Å—ã–ª–∫–∞–º\",\n",
    "    \"header_html\": \"\"\"\n",
    "            <div class=\"main-header\">\n",
    "                <h1 align=\"center\">FastFit: –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–æ—á–Ω–∞—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∏—Ä—É–µ–º—ã—Ö –¥–∏—Ñ—Ñ—É–∑–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.</h1>            \n",
    "                <div align=\"center\" style=\"display: flex; justify-content: center; align-items: center; margin: 20px 0;\">\n",
    "                    <a href=\"https://github.com/Zheng-Chong/FastFit/blob/main/LICENSE.md\" style=\"margin: 0 2px;\"><img src='https://img.shields.io/badge/License-NonCommercial-lightgreen?style=flat&logo=Lisence' alt='License'></a>\n",
    "                </div>\n",
    "            </div>\n",
    "    \"\"\",\n",
    "    \"input_settings\": \"<h3>üì§ –í–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–∏–º–µ—Ä–æ—á–Ω–∞—è</h3>\",\n",
    "    \"person_image_header\": \"<h4>üë§ –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è —á–µ–ª–æ–≤–µ–∫–∞</h4>\",\n",
    "    \"person_image_info\": '<p class=\"constraint-text\">üí° –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ–ª–Ω—ã–π —Ä–æ—Å—Ç —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —Å—Ç–æ—Ä–æ–Ω 3:4. –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –¥—Ä—É–≥–∏–º —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è–º–∏ —Å—Ç–æ—Ä–æ–Ω –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–µ–∑–∞–Ω—ã –ø–æ —Ü–µ–Ω—Ç—Ä—É.</p>',\n",
    "    \"person_image_label\": \"–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–µ—Ä–µ–æ–¥–µ—Ç—å.\",\n",
    "    \"ref_garment_header\": \"<h4>üëó –û–¥–µ–∂–¥–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∫–∏</h4>\",\n",
    "    \"ref_garment_info\": '<p class=\"constraint-text\">‚ö†Ô∏è –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –ø–ª–∞—Ç—å—è/–ø–∞–ª—å—Ç–æ/–∫–æ–º–±–∏–Ω–µ–∑–æ–Ω—ã –Ω–µ–ª—å–∑—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –æ–±—Ä–∞–∑–∞.</p>',\n",
    "    \"upper_body_label\": \"–†—É–±–∞—à–∫–∏/–±–ª—É–∑–∫–∏/—Ñ—É—Ç–±–æ–ª–∫–∏/—Ç–æ–ø—ã –∏ —Ç.–ø.\",\n",
    "    \"lower_body_label\": \"–ë—Ä—é–∫–∏/—é–±–∫–∏/—à–æ—Ä—Ç—ã –∏ —Ç.–ø.\",\n",
    "    \"dress_label\": \"–ü–ª–∞—Ç—å—è/–ö–æ–º–±–∏–Ω–µ–∑–æ–Ω—ã/–ü–∞–ª—å—Ç–æ\",\n",
    "    \"shoes_label\": \"–û–±—É–≤—å\",\n",
    "    \"bag_label\": \"–°—É–º–∫–∞\",\n",
    "    \"generate_params\": \"<h3>‚öôÔ∏è –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤</h3>\",\n",
    "    \"inference_settings\": \"<h4>üéÆ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã–≤–æ–¥–∞</h4>\",\n",
    "    \"ref_size_label\": \"–í—ã—Å–æ—Ç–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏\",\n",
    "    \"ref_size_info\": \"–ó–∞–¥–∞–π—Ç–µ –≤—ã—Å–æ—Ç—É —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏: 512/768/1024Ôºå–®–∏—Ä–∏–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å—Ç–æ—Ä–æ–Ω 3:4.\",\n",
    "    \"steps_label\": \"–ß–∏—Å–ª–æ —ç–ø–æ—Ö\",\n",
    "    \"steps_info\": \"–£–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ–∫ –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Ä–æ—Å—Ç—É –∫–∞—á–µ—Å—Ç–≤—É, –Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.\",\n",
    "    \"guidance_label\": \"CFG Scale\",\n",
    "    \"guidance_info\": \"–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–π—Ç–µ —Å—Ç–µ–ø–µ–Ω—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–æ–ª—É—á–µ–Ω–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é.\",\n",
    "    \"pose_guidance_label\": \"–£—á–µ—Å—Ç—å –ø–æ–∑—É –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\",\n",
    "    \"pose_guidance_info\": \"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ–∑–µ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\",\n",
    "    \"generate_button\": \"üöÄ –ù–∞—á–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é\",\n",
    "    \"status_text\": \"\",\n",
    "    \"result_header\": \"<h3>üì∏ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è....</h3>\",\n",
    "    \"output_image_label\": \"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\",\n",
    "    \"instructions_html\": \"\"\"\n",
    "        <div class=\"info-box\">\n",
    "            <h4>üí° –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏</h4>\n",
    "            <ul>\n",
    "                <li><strong>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏Ôºö</strong> –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–∞—Ç—å —á–µ—Ç–∫—É—é —Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é (—Ñ—Ä–æ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–∞–∫—É—Ä—Å)  \n",
    "                            –≤ –ø–æ–ª–Ω—ã–π —Ä–æ—Å—Ç —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º –Ω–µ –º–µ–Ω–µ–µ 768x1024 —Å —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —Å—Ç–æ—Ä–æ–Ω 3:4.</li>\n",
    "                <li><strong>–û–±—Ä–µ–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—èÔºö</strong> –§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –±—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–µ–∑–∞–Ω–∞ –ø–æ —Ü–µ–Ω—Ç—Ä—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏ –ø—Ä–æ–ø–æ—Ä—Ü–∏–π 3:4Ôºå\n",
    "                –ø—Ä–∏ —ç—Ç–æ–º –∫–∞–∫–∞—è-—Ç–æ —á–∞—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–æ–∂–µ—Ç –±—ã—Ç—å —É—Ç–µ—Ä—è–Ω–∞.</li>\n",
    "                <li><strong>–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∫–∏Ôºö</strong> –ü–ª–∞—Ç—å—è, –∫–æ–º–±–∏–Ω–µ–∑–æ–Ω—ã–µ –Ω–µ–ª—å–∑—è –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤–º–µ—Å—Ç–µ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–µ–¥–º–µ—Ç–∞–º–∏ –æ–¥–µ–∂–¥—ã.</li>\n",
    "                <li><strong>–†–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤Ôºö</strong> –†–∞–∑–º–µ—Ä —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –æ–¥–µ–∂–¥—ã; \n",
    "                —Å–Ω–∏–∂–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —á–∏—Å–ª–∞ —ç–ø–æ—Ö –ø–æ–≤—ã—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏; \n",
    "                —É–º–µ–Ω—å—à–µ–Ω–∏–µ CFG Scale —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç —Å–≤–æ–±–æ–¥—É –º–æ–¥–µ–ª–∏ –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞ —É–≤–µ–ª–µ—á–µ–Ω–∏–µ - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç</li>\n",
    "            </ul>\n",
    "            <h4>‚ö†Ô∏è –ú–µ—Ä—ã –ø—Ä–µ–¥–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç–∏</h4>\n",
    "            <ul>\n",
    "                <li>–í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –±—É–¥—å—Ç–µ —Ç–µ—Ä–ø–µ–ª–∏–≤—ã.</li>\n",
    "                <li>–ö–∞—á–µ—Å—Ç–≤–æ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏. \n",
    "                –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é –¥–ª—è –ø—Ä–∏–º–µ—Ä–∫–∏.</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    \"\"\",\n",
    "    \"validation_no_person\": \"‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—é —á–µ–ª–æ–≤–µ–∫–∞.\",\n",
    "    \"validation_dress_conflict\": \"‚ùå –ü–ª–∞—Ç—å—è, –∫–æ–º–±–∏–Ω–µ–∑–æ–Ω—ã, –ø–∞–ª—å—Ç–æ –Ω–µ–ª—å–∑—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å –¥—Ä—É–≥–∏–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –æ–¥–µ–∂–¥—ã.\",\n",
    "    \"validation_no_garment\": \"‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –æ–¥–µ–∂–¥—ã –¥–ª—è –ø—Ä–∏–º–µ—Ä–∫–∏.\",\n",
    "    \"validation_pass\": \"‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–π–¥–µ–Ω–∞\",\n",
    "    \"error_model_not_loaded\": \"‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å\",\n",
    "    \"error_pose_estimation_failed\": \"‚ùå –û—Ü–µ–Ω–∫–∞ –ø–æ–∑—ã –Ω–µ —É–¥–∞–ª–∞—Å—å\",\n",
    "    \"error_no_valid_person_image\": \"‚ùå –ù–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–µ–π —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —á–µ–ª–æ–≤–µ–∫–∞.\",\n",
    "    \"error_generation_failed\": \"‚ùå –°–±–æ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.\",\n",
    "    \"error_exception\": \"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏: {}\",\n",
    "    \"success_generation\": \"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æÔºÅ\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477efb99-4747-4abf-84ff-f9fb2af40e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop_to_aspect_ratio(img: Image.Image, target_ratio: float) -> Image.Image:\n",
    "    width, height = img.size\n",
    "    current_ratio = width / height\n",
    "    \n",
    "    if current_ratio > target_ratio:\n",
    "        new_width = int(height * target_ratio)\n",
    "        new_height = height\n",
    "        left = (width - new_width) // 2\n",
    "        top = 0\n",
    "    else:\n",
    "        new_width = width\n",
    "        new_height = int(width / target_ratio)\n",
    "        left = 0\n",
    "        top = (height - new_height) // 2\n",
    "    \n",
    "    return img.crop((left, top, left + new_width, top + new_height))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acd68550-30dc-4522-857e-9e39e22cae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FastFitDemo:\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model_path: str = \"Models/FastFit-MR-1024\", \n",
    "        util_model_path: str = \"Models/Human-Toolkit\",\n",
    "        mixed_precision: str = \"bf16\",\n",
    "        device: str = None,\n",
    "    ):\n",
    "        # If not exists, download the models\n",
    "        if not os.path.exists(base_model_path):\n",
    "            os.makedirs(base_model_path, exist_ok=True)\n",
    "            snapshot_download(\n",
    "                repo_id=\"zhengchong/FastFit-MR-1024\",\n",
    "                local_dir=base_model_path,\n",
    "                local_dir_use_symlinks=False\n",
    "            )\n",
    "        if not os.path.exists(util_model_path):\n",
    "            os.makedirs(util_model_path, exist_ok=True)\n",
    "            snapshot_download(\n",
    "                repo_id=\"zhengchong/Human-Toolkit\",\n",
    "                local_dir=util_model_path,\n",
    "                local_dir_use_symlinks=False\n",
    "            )\n",
    "            \n",
    "        self.device = device if device is not None else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dwpose_detector = DWposeDetector(pretrained_model_name_or_path=os.path.join(util_model_path, \"DWPose\"), device='cpu')\n",
    "        self.densepose_detector = DensePose(model_path=os.path.join(util_model_path, \"DensePose\"), device=self.device)\n",
    "        self.schp_lip_detector = SCHP(ckpt_path=os.path.join(util_model_path, \"SCHP\", \"schp-lip.pth\"), device=self.device)\n",
    "        self.schp_atr_detector = SCHP(ckpt_path=os.path.join(util_model_path, \"SCHP\", \"schp-atr.pth\"), device=self.device)\n",
    "        self.pipeline = FastFitPipeline(\n",
    "            base_model_path=base_model_path,\n",
    "            device=self.device,\n",
    "            mixed_precision=mixed_precision,\n",
    "            allow_tf32=True\n",
    "        )\n",
    "\n",
    "    def validate_inputs(self, person_img, upper_img, lower_img, dress_img, shoe_img, bag_img) -> Tuple[bool, str]:\n",
    "        # MODIFIED: Simplified check for person_img as it's now a direct Image, not a dict.\n",
    "        if person_img is None:\n",
    "            return False, \"validation_no_person\"\n",
    "        \n",
    "        has_upper = upper_img is not None\n",
    "        has_lower = lower_img is not None\n",
    "        has_dress = dress_img is not None\n",
    "        \n",
    "        if has_dress and (has_upper or has_lower):\n",
    "            return False, \"validation_dress_conflict\"\n",
    "        \n",
    "        if not (has_dress or has_upper or has_lower or shoe_img or bag_img):\n",
    "            return False, \"validation_no_garment\"\n",
    "        \n",
    "        return True, \"validation_pass\"\n",
    "    \n",
    "    def preprocess_person_image(self, person_img: Image.Image):\n",
    "        if self.dwpose_detector is None or self.densepose_detector is None or self.schp_lip_detector is None or self.schp_atr_detector is None:\n",
    "            raise RuntimeError(\"Model not initialized\")\n",
    "            \n",
    "        person_img = person_img.convert(\"RGB\")\n",
    "        person_img = center_crop_to_aspect_ratio(person_img, 3/4)\n",
    "        person_img = person_img.resize(PERSON_SIZE, Image.LANCZOS)\n",
    "        \n",
    "        pose_img = self.dwpose_detector(person_img)\n",
    "        if not isinstance(pose_img, Image.Image):\n",
    "            raise RuntimeError(\"Pose estimation failed\")\n",
    "        \n",
    "        densepose_arr = np.array(self.densepose_detector(person_img))\n",
    "        lip_arr = np.array(self.schp_lip_detector(person_img))\n",
    "        atr_arr = np.array(self.schp_atr_detector(person_img))\n",
    "        \n",
    "        return pose_img, densepose_arr, lip_arr, atr_arr\n",
    "    \n",
    "    def generate_mask(self, densepose_arr: np.ndarray, lip_arr: np.ndarray, atr_arr: np.ndarray) -> Image.Image:\n",
    "        return multi_ref_cloth_agnostic_mask(\n",
    "            densepose_arr, lip_arr, atr_arr,\n",
    "            horizon_expand=True\n",
    "        )\n",
    "    \n",
    "    def prepare_reference_images(self, upper_img, lower_img, dress_img, shoe_img, bag_img, ref_height: int) -> Tuple[List[Image.Image], List[str], List[int]]:\n",
    "        clothing_ref_size = (int(ref_height * 3 / 4), ref_height)\n",
    "        accessory_ref_size = (384, 512)\n",
    "        \n",
    "        ref_images, ref_labels, ref_attention_masks = [], [], []\n",
    "        \n",
    "        categories = [\n",
    "            (upper_img, \"upper\"), (lower_img, \"lower\"), (dress_img, \"overall\"),\n",
    "            (shoe_img, \"shoe\"), (bag_img, \"bag\")\n",
    "        ]\n",
    "        \n",
    "        for img, label in categories:\n",
    "            target_size = accessory_ref_size if label in [\"shoe\", \"bag\"] else clothing_ref_size\n",
    "            if img is not None:\n",
    "                img = img.convert(\"RGB\").resize(target_size, Image.LANCZOS)\n",
    "                ref_images.append(img)\n",
    "                ref_labels.append(label)\n",
    "                ref_attention_masks.append(1)\n",
    "            else:\n",
    "                ref_images.append(Image.new(\"RGB\", target_size, color=(0, 0, 0)))\n",
    "                ref_labels.append(label)\n",
    "                ref_attention_masks.append(0)\n",
    "        \n",
    "        return ref_images, ref_labels, ref_attention_masks\n",
    "    \n",
    "    def generate_image(\n",
    "        self, person_img, upper_img, lower_img, dress_img, shoe_img, bag_img,\n",
    "        ref_height: int, num_inference_steps: int = 50, guidance_scale: float = 2.5,\n",
    "        enable_pose: bool = True\n",
    "    ) -> Tuple[Optional[Image.Image], str]:\n",
    "        \n",
    "        try:\n",
    "            is_valid, message_key = self.validate_inputs(person_img, upper_img, lower_img, dress_img, shoe_img, bag_img)\n",
    "            if not is_valid:\n",
    "                return None, message_key\n",
    "            \n",
    "            if self.pipeline is None:\n",
    "                return None, \"error_model_not_loaded\"\n",
    "            \n",
    "            # MODIFIED: person_img is now a PIL.Image directly, so no dictionary handling is needed.\n",
    "            if person_img is None:\n",
    "                return None, \"error_no_valid_person_image\"\n",
    "\n",
    "            processed_person_img = person_img.convert(\"RGB\")\n",
    "            processed_person_img = center_crop_to_aspect_ratio(processed_person_img, 3/4)\n",
    "            processed_person_img = processed_person_img.resize(PERSON_SIZE, Image.LANCZOS)\n",
    "            \n",
    "            # This function does its own internal processing of the person image\n",
    "            pose_img, densepose_arr, lip_arr, atr_arr = self.preprocess_person_image(person_img)\n",
    "            \n",
    "            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –º–∞—Å–∫–∏\n",
    "            mask_img = self.generate_mask(densepose_arr, lip_arr, atr_arr)\n",
    "            \n",
    "            ref_images, ref_labels, ref_attention_masks = self.prepare_reference_images(\n",
    "                upper_img, lower_img, dress_img, shoe_img, bag_img, ref_height\n",
    "            )\n",
    "            \n",
    "            generator = torch.Generator(device=self.device).manual_seed(DEF_SEED)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                result = self.pipeline(\n",
    "                    person=processed_person_img, mask=mask_img, ref_images=ref_images,\n",
    "                    ref_labels=ref_labels, ref_attention_masks=ref_attention_masks,\n",
    "                    pose=pose_img if enable_pose else None, num_inference_steps=num_inference_steps,\n",
    "                    guidance_scale=guidance_scale, generator=generator, return_pil=True\n",
    "                )\n",
    "            \n",
    "            if isinstance(result, list) and len(result) > 0 and isinstance(result[0], Image.Image):\n",
    "                return result[0], \"success_generation\"\n",
    "            \n",
    "            return None, \"error_generation_failed\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An exception occurred: {e}\")\n",
    "            return None, f\"error_exception:{e}\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dacee21b-92e1-4a06-90ba-f52eddd62935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo():\n",
    "    demo_instance = FastFitDemo()\n",
    "    \n",
    "    with gr.Blocks(theme=\"soft\", css=\"\"\"\n",
    "        .main-header { text-align: center; margin-bottom: 2rem; }\n",
    "        .upload-section { border: 2px dashed #e0e0e0; border-radius: 10px; padding: 1rem; margin: 1rem 0; }\n",
    "        .constraint-text { color: #ff6b6b; font-size: 0.9em; font-style: italic; }\n",
    "        .info-box { background-color: var(--block-background-fill); border: 1px solid var(--border-color-primary); padding: 1rem; border-radius: 8px; margin-top: 2rem; }\n",
    "        @media (prefers-color-scheme: light) { .info-box { background-color: #f8f9fa; color: #343a40; border-color: #dee2e6; } }\n",
    "        @media (prefers-color-scheme: dark) { .info-box { background-color: rgba(255, 255, 255, 0.05); color: #e9ecef; border-color: rgba(255, 255, 255, 0.1); } }\n",
    "    \"\"\") as demo:\n",
    "        \n",
    "        \n",
    "        # --- UI Components ---\n",
    "        # We define all components here so they can be targeted by the update function\n",
    "        \n",
    "        # Header\n",
    "        header_html = gr.HTML(translations[\"header_html\"])\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                input_settings_header = gr.HTML(translations[\"input_settings\"])\n",
    "                with gr.Group():\n",
    "                    person_image_header = gr.HTML(translations[\"person_image_header\"])\n",
    "                    person_image_info = gr.HTML(translations[\"person_image_info\"])\n",
    "                    # MODIFIED: Changed from gr.ImageMask to gr.Image\n",
    "                    person_image = gr.Image(label=translations[\"person_image_label\"], type=\"pil\", sources=[\"upload\"], height=400)\n",
    "                \n",
    "                with gr.Group():\n",
    "                    ref_garment_header = gr.HTML(translations[\"ref_garment_header\"])\n",
    "                    ref_garment_info = gr.HTML(translations[\"ref_garment_info\"])\n",
    "                    with gr.Row():\n",
    "                        upper_image = gr.Image(label=translations[\"upper_body_label\"], type=\"pil\", sources=[\"upload\"], height=200)\n",
    "                        lower_image = gr.Image(label=translations[\"lower_body_label\"], type=\"pil\", sources=[\"upload\"], height=200)\n",
    "                    dress_image = gr.Image(label=translations[\"dress_label\"], type=\"pil\", sources=[\"upload\"], height=200)\n",
    "                    with gr.Row():\n",
    "                        shoe_image = gr.Image(label=translations[\"shoes_label\"], type=\"pil\", sources=[\"upload\"], height=200)\n",
    "                        bag_image = gr.Image(label=translations[\"bag_label\"], type=\"pil\", sources=[\"upload\"], height=200)\n",
    "\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "\n",
    "                generate_params_header = gr.HTML(translations[\"generate_params\"])\n",
    "                with gr.Group():\n",
    "                    inference_settings_header = gr.HTML(translations[\"inference_settings\"])\n",
    "                    ref_size = gr.Slider(minimum=512, maximum=1024, step=256, value=512, label=translations[\"ref_size_label\"], info=translations[\"ref_size_info\"])\n",
    "                    num_steps = gr.Slider(minimum=10, maximum=100, value=10, step=1, label=translations[\"steps_label\"], info=translations[\"steps_info\"])\n",
    "                    guidance_scale = gr.Slider(minimum=1.0, maximum=10.0, value=2.5, step=0.1, label=translations[\"guidance_label\"], info=translations[\"guidance_info\"])\n",
    "                    with gr.Row():\n",
    "                        enable_pose = gr.Checkbox(label=translations[\"pose_guidance_label\"], value=True, info=translations[\"pose_guidance_info\"])\n",
    "                \n",
    "                generate_btn = gr.Button(translations[\"generate_button\"], variant=\"primary\", size=\"lg\")\n",
    "                status_text = gr.HTML()\n",
    "                \n",
    "                result_header = gr.HTML(translations[\"result_header\"])\n",
    "                output_image = gr.Image(label=translations[\"output_image_label\"], type=\"pil\", height=400)\n",
    "        \n",
    "        instructions_html = gr.HTML(translations[\"instructions_html\"])\n",
    "        \n",
    "        # --- Event Handlers ---\n",
    "\n",
    "        \n",
    "        \n",
    "        def generation_wrapper(*args):\n",
    "            \"\"\"A wrapper to handle translation of status messages.\"\"\"\n",
    "            img, msg_key = demo_instance.generate_image(*args)\n",
    "            \n",
    "            t = translations\n",
    "            \n",
    "            # Handle exception messages with placeholders\n",
    "            if \"error_exception:\" in msg_key:\n",
    "                key, error_msg = msg_key.split(\":\", 1)\n",
    "                status_message = t[key].format(error_msg)\n",
    "            else:\n",
    "                status_message = t.get(msg_key, \"Unknown status\")\n",
    "\n",
    "            # Add color styling to the status message\n",
    "            if \"‚úÖ\" in status_message or \"successful\" in status_message:\n",
    "                styled_message = f'<p style=\"color: #51cf66;\">{status_message}</p>'\n",
    "            else:\n",
    "                styled_message = f'<p style=\"color: #ff6b6b;\">{status_message}</p>'\n",
    "\n",
    "            return img, styled_message\n",
    "\n",
    "        generate_btn.click(\n",
    "            fn=generation_wrapper,\n",
    "            inputs=[\n",
    "                person_image, upper_image, lower_image, dress_image, \n",
    "                shoe_image, bag_image, ref_size, num_steps, guidance_scale,\n",
    "                enable_pose\n",
    "            ],\n",
    "            outputs=[output_image, status_text]\n",
    "        )\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a099b1a6-ed7c-4840-8a1b-e1a7162f3022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while trying to fetch Models/FastFit-MR-1024: Error no file named diffusion_pytorch_model.safetensors found in directory Models/FastFit-MR-1024.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7870\n",
      "* Running on public URL: https://255ceb8b75b6c06ca5.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://255ceb8b75b6c06ca5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\envs\\fastfit\\lib\\site-packages\\torch\\functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:4316.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [07:55<00:00, 47.53s/it]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    demo = create_demo()\n",
    "    demo.launch(\n",
    "        server_name=\"0.0.0.0\",\n",
    "        server_port=7870,\n",
    "        share=True,\n",
    "        show_error=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb7401-cf3a-4dbe-b685-b1bad061b931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
